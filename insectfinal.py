# -*- coding: utf-8 -*-
"""Insectfinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15ue5zK7VGcfPCZ_vgrKjB7k-nmUZPqkf
"""

!pip install timm==0.6.13 torch torchvision

import torch
from torch import nn
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
import timm
import os

# Mount Google Drive if using for dataset storage
from google.colab import drive
drive.mount('/content/drive')

# Define dataset directory path (update it to the correct location of your dataset)
data_dir = '/content/drive/MyDrive/dataset'

# Define data augmentation and preprocessing
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.Resize((224, 224)),  # Resize images to PVT input size
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization
    ]),
    'val': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
}

dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms['train'])

# Split dataset into train and validation sets
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# Create DataLoader for training and validation sets
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)

class PatchEmbedding(nn.Module):
    def _init_(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96):
        super()._init_()
        self.img_size = img_size
        self.patch_size = patch_size
        self.grid_size = img_size // patch_size
        self.num_patches = self.grid_size ** 2

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.proj(x)
        x = x.flatten(2).transpose(1, 2)  # B, num_patches, embed_dim
        return x

# Load a pretrained Pyramid Vision Transformer model
model = timm.create_model('pvt_v2_b0', pretrained=True, num_classes=10)  # Change num_classes to match the insect species count

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

import torch
import time
from tqdm import tqdm

def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10, device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    best_accuracy = 0.0

    for epoch in range(num_epochs):
        start_time = time.time()  # Start time for the epoch

        # Training phase
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in tqdm(train_loader, desc=f'Training Epoch {epoch + 1}/{num_epochs}', leave=False):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            # Calculate accuracy for the training phase
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0

        with torch.no_grad():
            for inputs, labels in tqdm(val_loader, desc=f'Validating Epoch {epoch + 1}/{num_epochs}', leave=False):
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                val_correct += (predicted == labels).sum().item()

        epoch_accuracy = correct / total
        epoch_loss = running_loss / len(train_loader)
        val_accuracy = val_correct / len(val_loader.dataset)
        val_loss /= len(val_loader)

        # Calculate time taken for the epoch
        epoch_time = time.time() - start_time

        # Print formatted output
        print(f'Epoch {epoch + 1}/{num_epochs}')
        print(f'{len(train_loader)}/{len(train_loader)} ━━━━━━━━━━━━━━━━━━━━ {int(epoch_time)}s {int(epoch_time / len(train_loader))}s/step - '
              f'accuracy: {epoch_accuracy:.4f} - loss: {epoch_loss:.4f} - '
              f'val_accuracy: {val_accuracy:.4f} - val_loss: {val_loss:.4f}')

        # Save the best model
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            torch.save(model.state_dict(), 'best_pvt_model.pth')
            print(f"New best model saved with accuracy: {best_accuracy:.4f}")

    return best_accuracy

train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10,device=None)

best_model = timm.create_model('pvt_v2_b0', pretrained=False, num_classes=10)  # Adjust num_classes
best_model.load_state_dict(torch.load('best_pvt_model.pth'))
best_model = best_model.to(device)

!pip install timm==0.6.13 torch torchvision
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
import os

data_transforms = {
    'val': transforms.Compose([
        transforms.Resize((224, 224)),  # Same size used in PVT training
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization
    ])
}

dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms['val'])
dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)

resnet = models.resnet50(pretrained=True)

feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # Remove the last fully connected layer
feature_extractor = feature_extractor.to(device)
feature_extractor.eval()

features = []
labels = []
with torch.no_grad():  # No need to compute gradients for feature extraction
    for inputs, target in dataloader:
        inputs = inputs.to(device)

output = feature_extractor(inputs)  # Pass inputs through ResNet without the final layer
output = output.view(output.size(0), -1)

features.append(output.cpu())  # Store features on CPU
labels.append(target)

features = torch.cat(features, dim=0)
labels = torch.cat(labels, dim=0)

torch.save(features, 'extracted_features_resnet.pt')
torch.save(labels, 'extracted_labels_resnet.pt')

print("Feature extraction complete. Features saved to 'extracted_features_resnet.pt' and labels saved to 'extracted_labels_resnet.pt'.")

features_loaded = torch.load('extracted_features_resnet.pt')
labels_loaded = torch.load('extracted_labels_resnet.pt')

print(f"Features shape: {features_loaded.shape}")  # Expecting shape [num_images, 2048]
print(f"Labels shape: {labels_loaded.shape}")

print(f'Total dataset size: {len(dataset)}')
print(f'Number of images in training dataset: {len(train_dataset)}')
print(f'Number of images in validation dataset: {len(val_dataset)}')

num_classes = 10  # Adjust this to match the number of insect classes

best_model = timm.create_model('pvt_v2_b0', pretrained=False, num_classes=num_classes)
best_model.load_state_dict(torch.load('best_pvt_model.pth'))  # Ensure the filename matches
best_model = best_model.to(device)

def evaluate_model(model, val_loader):
    model.eval()  # Set the model to evaluation mode
    correct = torch.zeros(len(dataset.classes)).to(device)  # To store correct predictions per class
    total = torch.zeros(len(dataset.classes)).to(device)  # To store total samples per class

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)

            # Update correct and total counts
            for i in range(len(labels)):
                total[labels[i]] += 1
                if predicted[i] == labels[i]:
                    correct[labels[i]] += 1

    # Calculate accuracy for each class
    class_accuracy = correct / total
    return class_accuracy

# Evaluate the model and get class accuracies
class_accuracies = evaluate_model(best_model, val_loader)

# Print the class accuracies
class_names = dataset.classes  # Get the class names from the dataset
for idx, class_name in enumerate(class_names):
    print(f'Accuracy for {class_name}: {class_accuracies[idx].item() * 100:.2f}%')

# Overall accuracy
overall_accuracy = class_accuracies.sum() / len(class_names)
print(f'Overall accuracy: {overall_accuracy.item() * 100:.2f}%')

import torch
import matplotlib.pyplot as plt
from torchvision import transforms
from PIL import Image
import os

# Define dataset directory path (update it to the correct location of your dataset)
data_dir = '/content/drive/MyDrive/dataset'

# Sort the class names alphabetically
class_names = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])
print("class names:", class_names)


# Preprocessing transformation (same as used during training)
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Function to predict the insect and display image
def predict_insect(image_path, model, class_names):
    # Load and preprocess the image
    image = Image.open(image_path)
    input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension

    # Send the input to the same device as the model (GPU or CPU)
    input_tensor = input_tensor.to(device)

    # Forward pass to get model prediction
    with torch.no_grad():
        model.eval()
        outputs = model(input_tensor)

    # Get the predicted class index
    _, predicted_idx = torch.max(outputs, 1)
    predicted_class = class_names[predicted_idx.item()]

    # Display the image along with the predicted insect name
    plt.imshow(image)
    plt.title(f"Predicted Insect: {predicted_class}")
    plt.axis('off')  # Hide axes
    plt.show()

    # Print the predicted class name
    print(f"Predicted Insect: {predicted_class}")

# Example usage: Select an image from the validation dataset
import os
import random

# Define the directory containing the validation dataset
data_dir = '/content/drive/MyDrive/dataset'  # Update with your actual validation dataset path

# Automatically extract class names from the dataset directory
class_names = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]

# Randomly select a class
random_class = random.choice(class_names)

# Get the list of image files in the selected class directory
class_dir = os.path.join(data_dir, random_class)
image_files = os.listdir(class_dir)

# Randomly select an image file
random_image_file = random.choice(image_files)

# Construct the full path to the randomly selected image
val_image_path = os.path.join(class_dir, random_image_file)

print("Randomly selected image path:", val_image_path)

# Now you can use this path in your prediction function
predict_insect(val_image_path, best_model, class_names)
 # Update this to a real path in your validation dataset
predict_insect(val_image_path, best_model, class_names)

import os

# Define the path to the butterfly images
butterfly_dir = '/content/drive/MyDrive/dataset/Butterfly'

# List all files in the butterfly directory
butterfly_images = os.listdir(butterfly_dir)
print("Available butterfly images:", butterfly_images)

import os
import random
import torch
import matplotlib.pyplot as plt
from torchvision import transforms
from PIL import Image
import timm

# Mount Google Drive if using for dataset storage
from google.colab import drive
drive.mount('/content/drive')

# Define dataset directory path (update it to the correct location of your dataset)
data_dir = '/content/drive/My Drive/dataset'  # Update with your actual dataset path

# Automatically extract class names from the dataset directory
class_names = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])
print("Extracted class names:", class_names)

# Preprocessing transformation (same as used during training)
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization
])

# Function to predict the insect and display image
def predict_insect(image_path, model, class_names):
    # Load and preprocess the image
    image = Image.open(image_path).convert("RGB")  # Ensure image is in RGB format
    input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension

    # Send the input to the same device as the model (GPU or CPU)
    input_tensor = input_tensor.to(device)

    # Forward pass to get model prediction
    with torch.no_grad():
        model.eval()
        outputs = model(input_tensor)

    # Get the predicted class index
    _, predicted_idx = torch.max(outputs, 1)
    predicted_class = class_names[predicted_idx.item()]

    # Display the image along with the predicted insect name
    plt.imshow(image)
    plt.title(f"Predicted Insect: {predicted_class}")
    plt.axis('off')  # Hide axes
    plt.show()

    # Print the predicted class name
    print(f"Predicted Insect: {predicted_class}")

# Randomly select an image from the validation dataset
random_class = random.choice(class_names)
class_dir = os.path.join(data_dir, random_class)
image_files = os.listdir(class_dir)

# Ensure the selected class directory is not empty
if image_files:
    random_image_file = random.choice(image_files)
    val_image_path = os.path.join(class_dir, random_image_file)
    print("Randomly selected image path:", val_image_path)

    # Load the best saved model for future inference
    best_model = timm.create_model('pvt_v2_b0', pretrained=False, num_classes=len(class_names))  # Adjust num_classes
    best_model.load_state_dict(torch.load('best_pvt_model.pth'))
    best_model = best_model.to(device)

    # Call the prediction function
    predict_insect(val_image_path, best_model, class_names)
else:
    print(f"No images found in the class directory: {class_dir}")

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

def get_predictions_and_labels(model, val_loader):
    model.eval()  # Set the model to evaluation mode
    all_preds = []
    all_labels = []

    with torch.no_grad():  # Disable gradient calculation for faster inference
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)  # Get the predicted class
            all_preds.append(preds.cpu().numpy())  # Append predictions
            all_labels.append(labels.cpu().numpy())  # Append true labels

    all_preds = np.concatenate(all_preds)  # Combine predictions into one array
    all_labels = np.concatenate(all_labels)  # Combine labels into one array
    return all_preds, all_labels

def plot_confusion_matrix(model, val_loader):
    all_preds, all_labels = get_predictions_and_labels(model, val_loader)

    # Generate confusion matrix
    cm = confusion_matrix(all_labels, all_preds)

    # Plot confusion matrix
    plt.figure(figsize=(7, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=dataset.classes, yticklabels=dataset.classes)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

def print_classification_report(model, val_loader):
    all_preds, all_labels = get_predictions_and_labels(model, val_loader)

    # Generate classification report
    report = classification_report(all_labels, all_preds, target_names=dataset.classes)
    print("Classification Report:\n")
    print(report)

plot_confusion_matrix(best_model, val_loader)
print_classification_report(best_model, val_loader)

import torch
import matplotlib.pyplot as plt

def plot_accuracy_graph(train_accuracies, val_accuracies, num_epochs):
    epochs = range(1, num_epochs + 1)

    plt.figure(figsize=(10, 6))
    plt.plot(epochs, train_accuracies, label='Training Accuracy', marker='o')
    plt.plot(epochs, val_accuracies, label='Validation Accuracy', marker='o')

    plt.title('Training and Validation Accuracy over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    plt.show()

num_epochs = 10
train_accuracies, val_accuracies = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10,device=None)

from sklearn.metrics import precision_score, recall_score, f1_score

def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):
    best_accuracy = 0.0
    train_accuracies = []
    val_accuracies = []
    train_precisions = []
    val_precisions = []
    train_recalls = []
    val_recalls = []
    train_f1s = []
    val_f1s = []

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_loss = 0.0
        all_train_preds = []
        all_train_labels = []

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            all_train_preds.extend(predicted.cpu().numpy())
            all_train_labels.extend(labels.cpu().numpy())

        # Calculate training metrics
        epoch_accuracy = accuracy_score(all_train_labels, all_train_preds)
        epoch_precision = precision_score(all_train_labels, all_train_preds, average='weighted')
        epoch_recall = recall_score(all_train_labels, all_train_preds, average='weighted')
        epoch_f1 = f1_score(all_train_labels, all_train_preds, average='weighted')

        train_accuracies.append(epoch_accuracy)
        train_precisions.append(epoch_precision)
        train_recalls.append(epoch_recall)
        train_f1s.append(epoch_f1)

        # Validation phase
        model.eval()
        correct = 0
        total = 0
        all_val_preds = []
        all_val_labels = []

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                all_val_preds.extend(predicted.cpu().numpy())
                all_val_labels.extend(labels.cpu().numpy())

        # Calculate validation metrics
        val_accuracy = correct / total
        val_precision = precision_score(all_val_labels, all_val_preds, average='weighted')
        val_recall = recall_score(all_val_labels, all_val_preds, average='weighted')
        val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')

        val_accuracies.append(val_accuracy)
        val_precisions.append(val_precision)
        val_recalls.append(val_recall)
        val_f1s.append(val_f1)

        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, '
              f'Train Acc: {epoch_accuracy:.4f}, Val Acc: {val_accuracy:.4f}')

    return train_accuracies, val_accuracies, train_precisions, val_precisions, train_recalls, val_recalls, train_f1s, val_f1s

def plot_performance_graph(train_accuracies, val_accuracies, train_precisions, val_precisions,
                           train_recalls, val_recalls, train_f1s, val_f1s):
    plt.figure(figsize=(12, 8))

    # Accuracy
    plt.subplot(2, 2, 1)
    plt.plot(train_accuracies, label='Training Accuracy', marker='o')
    plt.plot(val_accuracies, label='Validation Accuracy', marker='o')
    plt.title('Accuracy over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid()

    # Precision
    plt.subplot(2, 2, 2)
    plt.plot(train_precisions, label='Training Precision', marker='o')
    plt.plot(val_precisions, label='Validation Precision', marker='o')
    plt.title('Precision over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Precision')
    plt.legend()
    plt.grid()

    # Recall
    plt.subplot(2, 2, 3)
    plt.plot(train_recalls, label='Training Recall', marker='o')
    plt.plot(val_recalls, label='Validation Recall', marker='o')
    plt.title('Recall over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Recall')
    plt.legend()
    plt.grid()

    # F1 Score
    plt.subplot(2, 2, 4)
    plt.plot(train_f1s, label='Training F1 Score', marker='o')
    plt.plot(val_f1s, label='Validation F1 Score', marker='o')
    plt.title('F1 Score over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('F1 Score')
    plt.legend()
    plt.grid()

    plt.tight_layout()
    plt.show()



def plot_accuracy_curve(train_accuracies, val_accuracies):
    epochs = range(1, len(train_accuracies) + 1)

    plt.plot(epochs, train_accuracies, 'bo-', label='Training Accuracy')
    plt.plot(epochs, val_accuracies, 'ro-', label='Validation Accuracy')

    plt.title('Training and Validation Accuracy per Epoch')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    plt.show()

# Call the plot function


def plot_loss_curve(train_losses, val_losses):
    epochs = range(1, len(train_losses) + 1)

    plt.plot(epochs, train_losses, 'bo-', label='Training Loss')
    plt.plot(epochs, val_losses, 'ro-', label='Validation Loss')

    plt.title('Training and Validation Loss per Epoch')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

# Example usage:
# Call the plot function



# After training the model, call the plot function with collected metrics
train_accuracies, val_accuracies, train_precisions, val_precisions, train_recalls, val_recalls, train_f1s, val_f1s = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)
plot_performance_graph(train_accuracies, val_accuracies, train_precisions, val_precisions, train_recalls, val_recalls, train_f1s, val_f1s)
plot_accuracy_curve(train_accuracies, val_accuracies)
plot_loss_curve(train_losses, val_losses)

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

# Define your model (this is just an example, replace it with your actual model)
class SimpleModel(nn.Module):
    def __init__(self, num_classes):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(16 * 112 * 112, num_classes)  # Adjust based on input size

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc1(x)
        return x

# Function to compute metrics
def compute_metrics(predictions, labels):
    precision = precision_score(labels, predictions, average='weighted')
    recall = recall_score(labels, predictions, average='weighted')
    f1 = f1_score(labels, predictions, average='weighted')
    return precision, recall, f1

# Training function
def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):
    best_accuracy = 0.0

    # Lists to store performance metrics
    train_accuracies = []
    val_accuracies = []
    train_precisions = []
    val_precisions = []
    train_recalls = []
    val_recalls = []
    train_f1s = []
    val_f1s = []
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        epoch_accuracy = correct / total
        train_accuracies.append(epoch_accuracy)
        train_losses.append(running_loss / len(train_loader))  # Append training loss

        # Validation phase
        model.eval()
        correct = 0
        total = 0
        val_loss = 0.0
        val_preds = []
        val_labels = []

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                val_preds.extend(predicted.cpu().numpy())
                val_labels.extend(labels.cpu().numpy())

        epoch_val_accuracy = correct / total
        val_accuracies.append(epoch_val_accuracy)
        val_losses.append(val_loss / len(val_loader))  # Append validation loss

        # Calculate precision, recall, F1 for validation
        precision, recall, f1 = compute_metrics(val_preds, val_labels)
        val_precisions.append(precision)
        val_recalls.append(recall)
        val_f1s.append(f1)

        # Print performance metrics
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, '
              f'Val Loss: {val_loss / len(val_loader):.4f}, Accuracy: {epoch_accuracy:.4f}, '
              f'Val Accuracy: {epoch_val_accuracy:.4f}')

        # Save the best model
        if epoch_val_accuracy > best_accuracy:
            best_accuracy = epoch_val_accuracy
            torch.save(model.state_dict(), 'best_model.pth')
            print(f"New best model saved with accuracy: {best_accuracy:.4f}")

    return train_accuracies, val_accuracies, train_precisions, val_precisions, train_recalls, val_recalls, train_f1s, val_f1s, train_losses, val_losses

def plot_performance_graph(train_accuracies, val_accuracies, train_precisions, val_precisions,
                           train_recalls, val_recalls, train_f1s, val_f1s):
    plt.figure(figsize=(12, 8))

    # Accuracy
    plt.subplot(2, 2, 1)
    plt.plot(train_accuracies, label='Training Accuracy', marker='o')
    plt.plot(val_accuracies, label='Validation Accuracy', marker='o')
    plt.title('Accuracy over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid()

    # Precision
    plt.subplot(2, 2, 2)
    plt.plot(train_precisions, label='Training Precision', marker='o')
    plt.plot(val_precisions, label='Validation Precision', marker='o')
    plt.title('Precision over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Precision')
    plt.legend()
    plt.grid()

    # Recall
    plt.subplot(2, 2, 3)
    plt.plot(train_recalls, label='Training Recall', marker='o')
    plt.plot(val_recalls, label='Validation Recall', marker='o')
    plt.title('Recall over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Recall')
    plt.legend()
    plt.grid()

    # F1 Score
    plt.subplot(2, 2, 4)
    plt.plot(train_f1s, label='Training F1 Score', marker='o')
    plt.plot(val_f1s, label='Validation F1 Score', marker='o')
    plt.title('F1 Score over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('F1 Score')
    plt.legend()
    plt.grid()

    plt.tight_layout()
    plt.show()

# After training the model, call the plot function with collected metrics
train_accuracies, val_accuracies, train_precisions, val_precisions, train_recalls, val_recalls, train_f1s, val_f1s = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)
plot_performance_graph(train_accuracies, val_accuracies, train_precisions, val_precisions, train_recalls, val_recalls, train_f1s, val_f1s)

def plot_performance_graph(train_accuracies, val_accuracies, train_precisions, val_precisions,
                           train_recalls, val_recalls, train_f1s, val_f1s):
    plt.figure(figsize=(12, 8))

    # Accuracy
    plt.subplot(2, 2, 1)
    plt.plot(train_accuracies, label='Training Accuracy', marker='o')
    plt.plot(val_accuracies, label='Validation Accuracy', marker='o')
    plt.title('Accuracy over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid()

    # Precision
    plt.subplot(2, 2, 2)
    plt.plot(train_precisions, label='Training Precision', marker='o')
    plt.plot(val_precisions, label='Validation Precision', marker='o')
    plt.title('Precision over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Precision')
    plt.legend()
    plt.grid()

    # Recall
    plt.subplot(2, 2, 3)
    plt.plot(train_recalls, label='Training Recall', marker='o')
    plt.plot(val_recalls, label='Validation Recall', marker='o')
    plt.title('Recall over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Recall')
    plt.legend()
    plt.grid()

    # F1 Score
    plt.subplot(2, 2, 4)
    plt.plot(train_f1s, label='Training F1 Score', marker='o')
    plt.plot(val_f1s, label='Validation F1 Score', marker='o')
    plt.title('F1 Score over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('F1 Score')
    plt.legend()
    plt.grid()

    plt.tight_layout()
    plt.show()

def plot_accuracy_curve(train_accuracies, val_accuracies):
    epochs = range(1, len(train_accuracies) + 1)

    plt.plot(epochs, train_accuracies, 'bo-', label='Training Accuracy')
    plt.plot(epochs, val_accuracies, 'ro-', label='Validation Accuracy')

    plt.title('Training and Validation Accuracy per Epoch')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    plt.show()

# Call the plot function


def plot_loss_curve(train_losses, val_losses):
    epochs = range(1, len(train_losses) + 1)

    plt.plot(epochs, train_losses, 'bo-', label='Training Loss')
    plt.plot(epochs, val_losses, 'ro-', label='Validation Loss')

    plt.title('Training and Validation Loss per Epoch')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score

def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):
    # Lists to store the metrics for each epoch
    train_accuracies = []
    val_accuracies = []
    train_precisions = []
    val_precisions = []
    train_recalls = []
    val_recalls = []
    train_f1s = []
    val_f1s = []

    for epoch in range(num_epochs):
        model.train()
        correct_train = 0
        total_train = 0
        all_train_preds = []
        all_train_labels = []

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            _, preds = torch.max(outputs, 1)
            correct_train += (preds == labels).sum().item()
            total_train += labels.size(0)

            all_train_preds.extend(preds.cpu().numpy())
            all_train_labels.extend(labels.cpu().numpy())

        # Calculate training accuracy
        train_accuracy = correct_train / total_train
        train_accuracies.append(train_accuracy)

        # Calculate precision, recall, and F1 score for training
        train_precision = precision_score(all_train_labels, all_train_preds, average='macro')
        train_recall = recall_score(all_train_labels, all_train_preds, average='macro')
        train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')

        train_precisions.append(train_precision)
        train_recalls.append(train_recall)
        train_f1s.append(train_f1)

        # Validation phase
        model.eval()
        correct_val = 0
        total_val = 0
        all_val_preds = []
        all_val_labels = []

        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, preds = torch.max(outputs, 1)
                correct_val += (preds == labels).sum().item()
                total_val += labels.size(0)

                all_val_preds.extend(preds.cpu().numpy())
                all_val_labels.extend(labels.cpu().numpy())

        # Calculate validation accuracy
        val_accuracy = correct_val / total_val
        val_accuracies.append(val_accuracy)

        # Calculate precision, recall, and F1 score for validation
        val_precision = precision_score(all_val_labels, all_val_preds, average='macro')
        val_recall = recall_score(all_val_labels, all_val_preds, average='macro')
        val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')

        val_precisions.append(val_precision)
        val_recalls.append(val_recall)
        val_f1s.append(val_f1)

        print(f'Epoch {epoch + 1}/{num_epochs} - '
              f'Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}, '
              f'Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}')

    return train_accuracies, val_accuracies, train_precisions, val_precisions, train_recalls, val_recalls, train_f1s, val_f1s

from sklearn.metrics import precision_score, recall_score, f1_score

def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):
    # Lists to store the metrics for each epoch
    train_accuracies = []
    val_accuracies = []
    train_precisions = []
    val_precisions = []
    train_recalls = []
    val_recalls = []
    train_f1s = []
    val_f1s = []
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        model.train()
        correct_train = 0
        total_train = 0
        running_train_loss = 0.0
        all_train_preds = []
        all_train_labels = []

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # Accumulate the training loss
            running_train_loss += loss.item()

            _, preds = torch.max(outputs, 1)
            correct_train += (preds == labels).sum().item()
            total_train += labels.size(0)

            all_train_preds.extend(preds.cpu().numpy())
            all_train_labels.extend(labels.cpu().numpy())

        # Calculate training loss and accuracy
        train_loss = running_train_loss / len(train_loader)
        train_losses.append(train_loss)

        train_accuracy = correct_train / total_train
        train_accuracies.append(train_accuracy)

        # Calculate precision, recall, and F1 score for training
        train_precision = precision_score(all_train_labels, all_train_preds, average='macro')
        train_recall = recall_score(all_train_labels, all_train_preds, average='macro')
        train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')

        train_precisions.append(train_precision)
        train_recalls.append(train_recall)
        train_f1s.append(train_f1)

        # Validation phase
        model.eval()
        correct_val = 0
        total_val = 0
        running_val_loss = 0.0
        all_val_preds = []
        all_val_labels = []

        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                # Accumulate validation loss
                running_val_loss += loss.item()

                _, preds = torch.max(outputs, 1)
                correct_val += (preds == labels).sum().item()
                total_val += labels.size(0)

                all_val_preds.extend(preds.cpu().numpy())
                all_val_labels.extend(labels.cpu().numpy())

        # Calculate validation loss and accuracy
        val_loss = running_val_loss / len(val_loader)
        val_losses.append(val_loss)

        val_accuracy = correct_val / total_val
        val_accuracies.append(val_accuracy)

        # Calculate precision, recall, and F1 score for validation
        val_precision = precision_score(all_val_labels, all_val_preds, average='macro')
        val_recall = recall_score(all_val_labels, all_val_preds, average='macro')
        val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')

        val_precisions.append(val_precision)
        val_recalls.append(val_recall)
        val_f1s.append(val_f1)

        print(f'Epoch {epoch + 1}/{num_epochs} - '
              f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '
              f'Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}')

    return (train_accuracies, val_accuracies, train_precisions, val_precisions,
            train_recalls, val_recalls, train_f1s, val_f1s, train_losses, val_losses)

# After training the model, call the plot function with collected metrics
train_accuracies, val_accuracies, train_precisions, val_precisions, train_recalls, val_recalls, train_f1s, val_f1s = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)

# Plot performance graphs
plot_performance_graph(train_accuracies, val_accuracies, train_precisions, val_precisions, train_recalls, val_recalls, train_f1s, val_f1s)
plot_accuracy_curve(train_accuracies, val_accuracies)
plot_loss_curve(train_losses, val_losses)

# Train the model and collect all metrics including losses
train_accuracies, val_accuracies, train_precisions, val_precisions, train_recalls, val_recalls, train_f1s, val_f1s, train_losses, val_losses = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)

# Plot performance graphs (Accuracy, Precision, Recall, F1 Score)
plot_performance_graph(train_accuracies, val_accuracies, train_precisions, val_precisions, train_recalls, val_recalls, train_f1s, val_f1s)

# Plot accuracy curve
plot_accuracy_curve(train_accuracies, val_accuracies)

# Plot loss curve
plot_loss_curve(train_losses, val_losses)

def evaluate_model(model, val_loader):
    model.eval()  # Set the model to evaluation mode
    correct = torch.zeros(len(dataset.classes)).to(device)  # To store correct predictions per class
    total = torch.zeros(len(dataset.classes)).to(device)  # To store total samples per class

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)

            # Update correct and total counts
            for i in range(len(labels)):
                total[labels[i]] += 1
                if predicted[i] == labels[i]:
                    correct[labels[i]] += 1

    # Calculate accuracy for each class
    class_accuracy = correct / total
    return class_accuracy

# Evaluate the model and get class accuracies
class_accuracies = evaluate_model(best_model, val_loader)

# Print the class accuracies
class_names = dataset.classes  # Get the class names from the dataset
for idx, class_name in enumerate(class_names):
    print(f'Accuracy for {class_name}: {class_accuracies[idx].item() * 100:.2f}%')

# Overall accuracy
overall_accuracy = class_accuracies.sum() / len(class_names)
print(f'Overall accuracy: {overall_accuracy.item() * 100:.2f}%')

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
def get_predictions_and_labels(model, val_loader):
    model.eval()  # Set the model to evaluation mode
    all_preds = []
    all_labels = []

    with torch.no_grad():  # Disable gradient calculation for faster inference
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)  # Get the predicted class
            all_preds.append(preds.cpu().numpy())  # Append predictions
            all_labels.append(labels.cpu().numpy())  # Append true labels

    all_preds = np.concatenate(all_preds)  # Combine predictions into one array
    all_labels = np.concatenate(all_labels)  # Combine labels into one array
    return all_preds, all_labels
def plot_confusion_matrix(model, val_loader):
    all_preds, all_labels = get_predictions_and_labels(model, val_loader)

    # Generate confusion matrix
    cm = confusion_matrix(all_labels, all_preds)

    # Plot confusion matrix
    plt.figure(figsize=(7, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=dataset.classes, yticklabels=dataset.classes)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()
def print_classification_report(model, val_loader):
    all_preds, all_labels = get_predictions_and_labels(model, val_loader)

    # Generate classification report
    report = classification_report(all_labels, all_preds, target_names=dataset.classes)
    print("Classification Report:\n")
    print(report)

plot_confusion_matrix(best_model, val_loader)
print_classification_report(best_model, val_loader)